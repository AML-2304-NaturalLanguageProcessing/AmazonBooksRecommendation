{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11d14821",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:57:19.940651Z",
     "iopub.status.busy": "2024-08-08T16:57:19.940344Z",
     "iopub.status.idle": "2024-08-08T16:57:23.590244Z",
     "shell.execute_reply": "2024-08-08T16:57:23.589300Z"
    },
    "papermill": {
     "duration": 3.658805,
     "end_time": "2024-08-08T16:57:23.592408",
     "exception": false,
     "start_time": "2024-08-08T16:57:19.933603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.2\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "434c2434",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-08T16:57:23.604642Z",
     "iopub.status.busy": "2024-08-08T16:57:23.604251Z",
     "iopub.status.idle": "2024-08-08T16:57:24.480082Z",
     "shell.execute_reply": "2024-08-08T16:57:24.478967Z"
    },
    "papermill": {
     "duration": 0.884776,
     "end_time": "2024-08-08T16:57:24.482777",
     "exception": false,
     "start_time": "2024-08-08T16:57:23.598001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/cdae/pytorch/default/1/cdae.py\n",
      "/kaggle/input/models/cdae.py\n",
      "/kaggle/input/cdae-model/user_book_matrix.npz\n",
      "/kaggle/input/cdae-model/avg_embeddings_matrix.npz\n",
      "/kaggle/input/cdae-model/emotion_matrix.npz\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "519b13b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:57:24.495409Z",
     "iopub.status.busy": "2024-08-08T16:57:24.494937Z",
     "iopub.status.idle": "2024-08-08T16:57:24.748186Z",
     "shell.execute_reply": "2024-08-08T16:57:24.747246Z"
    },
    "papermill": {
     "duration": 0.262308,
     "end_time": "2024-08-08T16:57:24.750693",
     "exception": false,
     "start_time": "2024-08-08T16:57:24.488385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#loading libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.sparse import load_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a50111b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:57:24.762984Z",
     "iopub.status.busy": "2024-08-08T16:57:24.762675Z",
     "iopub.status.idle": "2024-08-08T16:57:24.767388Z",
     "shell.execute_reply": "2024-08-08T16:57:24.766556Z"
    },
    "papermill": {
     "duration": 0.012958,
     "end_time": "2024-08-08T16:57:24.769230",
     "exception": false,
     "start_time": "2024-08-08T16:57:24.756272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    user_book_matrix = load_npz('/kaggle/input/cdae-model/user_book_matrix.npz')\n",
    "    emotion_matrix = load_npz('/kaggle/input/cdae-model/emotion_matrix.npz')\n",
    "    book_embeddings = load_npz('/kaggle/input/cdae-model/avg_embeddings_matrix.npz')\n",
    "\n",
    "    return user_book_matrix, emotion_matrix, book_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12d0da4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:57:24.780916Z",
     "iopub.status.busy": "2024-08-08T16:57:24.780656Z",
     "iopub.status.idle": "2024-08-08T16:57:24.790239Z",
     "shell.execute_reply": "2024-08-08T16:57:24.789370Z"
    },
    "papermill": {
     "duration": 0.017489,
     "end_time": "2024-08-08T16:57:24.792007",
     "exception": false,
     "start_time": "2024-08-08T16:57:24.774518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class CDAEDataset(Dataset):\n",
    "    def __init__(self, interactions, emotions, review_embeddings, noise_factor=0.2):\n",
    "        self.users, self.items = interactions.nonzero()\n",
    "        self.interactions = interactions.toarray()\n",
    "        self.emotions = emotions.toarray()\n",
    "        self.review_embeddings = review_embeddings.toarray()\n",
    "        self.noise_factor = noise_factor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user, item = self.users[idx], self.items[idx]\n",
    "        user_vector = self.interactions[user].flatten()\n",
    "        noisy_user_vector = self.add_noise(user_vector)\n",
    "        emotion_vector = torch.tensor(self.emotions[item], dtype=torch.float32)\n",
    "        review_embedding = torch.tensor(self.review_embeddings[item], dtype=torch.float32)\n",
    "        \n",
    "        return (torch.FloatTensor(noisy_user_vector), \n",
    "                torch.FloatTensor(user_vector),\n",
    "                emotion_vector, \n",
    "                review_embedding,\n",
    "                torch.LongTensor([user]))\n",
    "\n",
    "    def add_noise(self, vector):\n",
    "        noise = np.random.normal(loc=0, scale=self.noise_factor, size=vector.shape)\n",
    "        noisy_vector = vector + noise\n",
    "        return np.clip(noisy_vector, 0, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad1b7bcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:57:24.803898Z",
     "iopub.status.busy": "2024-08-08T16:57:24.803637Z",
     "iopub.status.idle": "2024-08-08T16:57:24.818312Z",
     "shell.execute_reply": "2024-08-08T16:57:24.817486Z"
    },
    "papermill": {
     "duration": 0.022811,
     "end_time": "2024-08-08T16:57:24.820099",
     "exception": false,
     "start_time": "2024-08-08T16:57:24.797288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CDAE(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_emotions, review_embedding_dim, \n",
    "                 embedding_dim=64, hidden_dims=[256, 128, 64], dropout=0.2):\n",
    "        super(CDAE, self).__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        self.emotion_embedding = nn.Embedding(num_emotions, embedding_dim)\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder = nn.ModuleList()\n",
    "        input_dim = num_items + embedding_dim * 3 + review_embedding_dim  # input + user + item + emotion + review\n",
    "        encoder_dims = [input_dim] + hidden_dims\n",
    "        for i in range(len(encoder_dims) - 1):\n",
    "            self.encoder.append(nn.Linear(encoder_dims[i], encoder_dims[i+1]))\n",
    "            self.encoder.append(nn.ReLU())\n",
    "            self.encoder.append(nn.BatchNorm1d(encoder_dims[i+1]))\n",
    "            self.encoder.append(nn.Dropout(dropout))\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder = nn.ModuleList()\n",
    "        decoder_dims = hidden_dims[::-1] + [num_items]\n",
    "        for i in range(len(decoder_dims) - 1):\n",
    "            self.decoder.append(nn.Linear(decoder_dims[i], decoder_dims[i+1]))\n",
    "            if i < len(decoder_dims) - 2:  # No activation on the final layer\n",
    "                self.decoder.append(nn.ReLU())\n",
    "                self.decoder.append(nn.BatchNorm1d(decoder_dims[i+1]))\n",
    "                self.decoder.append(nn.Dropout(dropout))\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, user_vector, user_indices, item_indices, emotion_weights, review_embeddings):\n",
    "        user_emb = self.user_embedding(user_indices)\n",
    "        item_emb = self.item_embedding(item_indices)\n",
    "        \n",
    "        # Handle emotion embeddings\n",
    "        emotion_emb = torch.matmul(emotion_weights, self.emotion_embedding.weight)\n",
    "\n",
    "        # Concatenate input\n",
    "        x = torch.cat([user_vector, user_emb, item_emb, emotion_emb, review_embeddings], dim=1)\n",
    "\n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Encode\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "        \n",
    "        encoded = x  # Save the encoded representation\n",
    "        \n",
    "        # Decode\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Apply sigmoid to get probabilities\n",
    "        reconstructed = self.sigmoid(x)\n",
    "        \n",
    "        return reconstructed, encoded\n",
    "\n",
    "    def get_recommendation(self, user_indices, item_indices, emotion_indices, review_embeddings):\n",
    "        with torch.no_grad():\n",
    "            # Use a zero vector as input (we're not denoising here, just reconstructing)\n",
    "            user_vector = torch.zeros(user_indices.size(0), self.item_embedding.num_embeddings).to(user_indices.device)\n",
    "            \n",
    "            reconstructed, _ = self.forward(user_vector, user_indices, item_indices, emotion_indices, review_embeddings)\n",
    "            \n",
    "            return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e7f7189",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:57:24.832026Z",
     "iopub.status.busy": "2024-08-08T16:57:24.831730Z",
     "iopub.status.idle": "2024-08-08T16:57:24.837042Z",
     "shell.execute_reply": "2024-08-08T16:57:24.836224Z"
    },
    "papermill": {
     "duration": 0.013272,
     "end_time": "2024-08-08T16:57:24.838835",
     "exception": false,
     "start_time": "2024-08-08T16:57:24.825563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_info(user_item_interactions, emotion_labels, review_embeddings):\n",
    "    print(f\"User-Item Interactions shape: {user_item_interactions.shape}\")\n",
    "    print(f\"Emotion Labels shape: {emotion_labels.shape}\")\n",
    "    print(f\"Review Embeddings shape: {review_embeddings.shape}\")\n",
    "    # Get the number of unique users and items\n",
    "    num_users = user_item_interactions.shape[0]\n",
    "    num_items = user_item_interactions.shape[1]\n",
    "    \n",
    "    # Get the number of unique emotions\n",
    "    num_emotions = emotion_labels.shape[1]\n",
    "    \n",
    "    # Get the dimension of review embeddings\n",
    "    review_embedding_dim = review_embeddings.shape[1]\n",
    "    \n",
    "    return num_users, num_items, num_emotions, review_embedding_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17fae8be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:57:24.850407Z",
     "iopub.status.busy": "2024-08-08T16:57:24.850150Z",
     "iopub.status.idle": "2024-08-08T16:57:24.855154Z",
     "shell.execute_reply": "2024-08-08T16:57:24.854399Z"
    },
    "papermill": {
     "duration": 0.012809,
     "end_time": "2024-08-08T16:57:24.856943",
     "exception": false,
     "start_time": "2024-08-08T16:57:24.844134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    noisy_user_vectors, user_vectors, emotions, review_embeddings, user_indices = zip(*batch)\n",
    "    \n",
    "    noisy_user_vectors = torch.stack(noisy_user_vectors)\n",
    "    user_vectors = torch.stack(user_vectors)\n",
    "    emotions = torch.stack(emotions)\n",
    "    review_embeddings = torch.stack(review_embeddings)\n",
    "    user_indices = torch.cat(user_indices)\n",
    "    \n",
    "    return noisy_user_vectors, user_vectors, emotions, review_embeddings, user_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9e60c24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:57:24.869218Z",
     "iopub.status.busy": "2024-08-08T16:57:24.868888Z",
     "iopub.status.idle": "2024-08-08T16:57:33.808223Z",
     "shell.execute_reply": "2024-08-08T16:57:33.807441Z"
    },
    "papermill": {
     "duration": 8.948515,
     "end_time": "2024-08-08T16:57:33.810774",
     "exception": false,
     "start_time": "2024-08-08T16:57:24.862259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Item Interactions shape: (575887, 74298)\n",
      "Emotion Labels shape: (74298, 6)\n",
      "Review Embeddings shape: (74298, 100)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "\n",
    "embedding_dim = 32\n",
    "review_embedding_dim = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# Load and preprocess data\n",
    "user_item_interactions, emotion_labels, review_embeddings = load_data()\n",
    "num_users, num_items, num_emotions, review_embedding_dim = get_data_info(user_item_interactions, emotion_labels, review_embeddings)\n",
    "\n",
    "# Create dataset\n",
    "dataset = CDAEDataset(user_item_interactions, emotion_labels, review_embeddings, noise_factor=0.2)\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Initialize the model\n",
    "model = CDAE(num_users, num_items, num_emotions, review_embedding_dim, \n",
    "             embedding_dim=embedding_dim, hidden_dims=[256, 128, 64])\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f6da94d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:57:33.823816Z",
     "iopub.status.busy": "2024-08-08T16:57:33.823035Z",
     "iopub.status.idle": "2024-08-08T16:57:33.831229Z",
     "shell.execute_reply": "2024-08-08T16:57:33.830376Z"
    },
    "papermill": {
     "duration": 0.016734,
     "end_time": "2024-08-08T16:57:33.833247",
     "exception": false,
     "start_time": "2024-08-08T16:57:33.816513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for noisy_user_vectors, user_vectors, emotion_weights, review_embeddings, user_indices in train_loader:\n",
    "        noisy_user_vectors = noisy_user_vectors.to(device)\n",
    "        user_vectors = user_vectors.to(device)\n",
    "        emotion_weights = emotion_weights.to(device)\n",
    "        review_embeddings = review_embeddings.to(device)\n",
    "        user_indices = user_indices.to(device)\n",
    "    \n",
    "        # Generate item indices (assuming they're the same as user indices in this case)\n",
    "        item_indices = user_indices.clone()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            prediction, _ = model(noisy_user_vectors, user_indices, item_indices, emotion_weights, review_embeddings)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(f\"Max user index: {user_indices.max().item()}\")\n",
    "            print(f\"Max item index: {item_indices.max().item()}\")\n",
    "            print(f\"Emotion weights shape: {emotion_weights.shape}\")\n",
    "            continue  # Skip this batch\n",
    "    \n",
    "        loss = criterion(prediction, user_vectors)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    average_loss = running_loss / len(dataloader)\n",
    "    return average_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2aaaf59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:57:33.845591Z",
     "iopub.status.busy": "2024-08-08T16:57:33.844882Z",
     "iopub.status.idle": "2024-08-08T16:57:33.851229Z",
     "shell.execute_reply": "2024-08-08T16:57:33.850489Z"
    },
    "papermill": {
     "duration": 0.014502,
     "end_time": "2024-08-08T16:57:33.853094",
     "exception": false,
     "start_time": "2024-08-08T16:57:33.838592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for noisy_user_vectors, user_vectors, emotions, review_embeddings, user_indices in dataloader:\n",
    "            noisy_user_vectors = noisy_user_vectors.to(device)\n",
    "            user_vectors = user_vectors.to(device)\n",
    "            emotions = emotions.to(device)\n",
    "            review_embeddings = review_embeddings.to(device)\n",
    "            user_indices = user_indices.to(device)\n",
    "\n",
    "            outputs, _ = model(noisy_user_vectors, user_indices, user_indices, user_indices, review_embeddings)\n",
    "            loss = criterion(outputs, user_vectors)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    average_loss = running_loss / len(dataloader)\n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb311d4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:57:33.865146Z",
     "iopub.status.busy": "2024-08-08T16:57:33.864458Z",
     "iopub.status.idle": "2024-08-08T16:57:33.874099Z",
     "shell.execute_reply": "2024-08-08T16:57:33.873391Z"
    },
    "papermill": {
     "duration": 0.017512,
     "end_time": "2024-08-08T16:57:33.875870",
     "exception": false,
     "start_time": "2024-08-08T16:57:33.858358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0, path='cdae_checkpoint.pth'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Saves model when validation loss decreases.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving CDAE model...')\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "    def load_checkpoint(self, model):\n",
    "        \"\"\"Loads the best saved model.\"\"\"\n",
    "        checkpoint = torch.load(self.path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        return checkpoint['val_loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ece9599f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:57:33.888059Z",
     "iopub.status.busy": "2024-08-08T16:57:33.887428Z",
     "iopub.status.idle": "2024-08-08T16:57:34.554746Z",
     "shell.execute_reply": "2024-08-08T16:57:34.553809Z"
    },
    "papermill": {
     "duration": 0.675918,
     "end_time": "2024-08-08T16:57:34.557125",
     "exception": false,
     "start_time": "2024-08-08T16:57:33.881207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize model\n",
    "model = CDAE(num_users, num_items, num_emotions, review_embedding_dim, \n",
    "             embedding_dim=32, hidden_dims=[256, 128, 64], dropout=0.2).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00f6aa9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T16:57:34.570132Z",
     "iopub.status.busy": "2024-08-08T16:57:34.569722Z",
     "iopub.status.idle": "2024-08-08T16:57:35.966312Z",
     "shell.execute_reply": "2024-08-08T16:57:35.964912Z"
    },
    "papermill": {
     "duration": 1.405262,
     "end_time": "2024-08-08T16:57:35.968140",
     "exception": true,
     "start_time": "2024-08-08T16:57:34.562878",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     prediction, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_user_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memotions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreview_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m, in \u001b[0;36mCDAE.forward\u001b[0;34m(self, user_vector, user_indices, item_indices, emotion_weights, review_embeddings)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Handle emotion embeddings\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m emotion_emb \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43memotion_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memotion_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Concatenate input\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser indices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_indices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mItem indices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_indices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmotion indices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00memotions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:934\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[0;32m--> 934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_spec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:133\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfloating_dtype:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m tensor_view:\n\u001b[0;32m--> 133\u001b[0m         value_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:933\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[0;32m--> 933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize EarlyStopping object\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True, path='/kaggle/working/cdaemodel.pth')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for noisy_user_vectors, user_vectors, emotions, review_embeddings, user_indices in train_loader:\n",
    "        noisy_user_vectors = noisy_user_vectors.to(device)\n",
    "        user_vectors = user_vectors.to(device)\n",
    "        emotions = emotions.to(device)\n",
    "        review_embeddings = review_embeddings.to(device)\n",
    "        user_indices = user_indices.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            prediction, _ = model(noisy_user_vectors, user_indices, user_indices, emotions, review_embeddings)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(f\"User indices: {user_indices}\")\n",
    "            print(f\"Item indices: {user_indices}\")\n",
    "            print(f\"Emotion indices: {emotions}\")\n",
    "            continue  # Skip this batch\n",
    "        \n",
    "        loss = criterion(prediction, user_vectors)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Compute average training loss\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for noisy_user_vectors, user_vectors, emotions, review_embeddings, user_indices in test_loader:\n",
    "            noisy_user_vectors = noisy_user_vectors.to(device)\n",
    "            user_vectors = user_vectors.to(device)\n",
    "            emotions = emotions.to(device)\n",
    "            review_embeddings = review_embeddings.to(device)\n",
    "            user_indices = user_indices.to(device)\n",
    "\n",
    "            prediction, _ = model(noisy_user_vectors, user_indices, user_indices, emotions, review_embeddings)\n",
    "            loss = criterion(prediction, user_vectors)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(test_loader)\n",
    "\n",
    "    # Print training and validation loss\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Update early stopping object\n",
    "    early_stopping(val_loss, model)\n",
    "\n",
    "    # Check if early stopping criterion is met\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "\n",
    "# Load the best model checkpoint\n",
    "model.load_state_dict(torch.load('/kaggle/working/cdaemodel.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8e0152",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-06T21:08:05.150889Z",
     "iopub.status.idle": "2024-08-06T21:08:05.151330Z",
     "shell.execute_reply": "2024-08-06T21:08:05.151125Z",
     "shell.execute_reply.started": "2024-08-06T21:08:05.151106Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Number of users: {num_users}\")\n",
    "print(f\"Number of items: {num_items}\")\n",
    "print(f\"Number of emotions: {num_emotions}\")\n",
    "print(f\"User embedding size: {model.user_embedding.num_embeddings}\")\n",
    "print(f\"Item embedding size: {model.item_embedding.num_embeddings}\")\n",
    "print(f\"Emotion embedding size: {model.emotion_embedding.num_embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e6540",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-06T21:08:05.152847Z",
     "iopub.status.idle": "2024-08-06T21:08:05.153143Z",
     "shell.execute_reply": "2024-08-06T21:08:05.153006Z",
     "shell.execute_reply.started": "2024-08-06T21:08:05.152994Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Max user index: {user_indices.max().item()}\")\n",
    "print(f\"Max item index: {item_indices.max().item()}\")\n",
    "print(f\"Max emotion index: {emotions.max().item()}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5480558,
     "sourceId": 9083525,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5480872,
     "sourceId": 9083967,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 97902,
     "modelInstanceId": 73022,
     "sourceId": 86942,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20.610702,
   "end_time": "2024-08-08T16:57:37.796711",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-08T16:57:17.186009",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
