{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8970098,"sourceType":"datasetVersion","datasetId":5399116}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-16T22:41:04.876550Z","iopub.execute_input":"2024-07-16T22:41:04.877575Z","iopub.status.idle":"2024-07-16T22:41:05.294724Z","shell.execute_reply.started":"2024-07-16T22:41:04.877537Z","shell.execute_reply":"2024-07-16T22:41:05.293619Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/readyforrecommendationmodel/user_book_matrix.npz\n/kaggle/input/readyforrecommendationmodel/avg_embeddings_matrix.npz\n/kaggle/input/readyforrecommendationmodel/emotion_matrix.npz\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2024-07-16T22:41:05.296742Z","iopub.execute_input":"2024-07-16T22:41:05.297179Z","iopub.status.idle":"2024-07-16T22:41:08.900886Z","shell.execute_reply.started":"2024-07-16T22:41:05.297152Z","shell.execute_reply":"2024-07-16T22:41:08.899966Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from scipy.sparse import load_npz\n\ndef load_data():\n    user_book_matrix = load_npz('/kaggle/input/readyforrecommendationmodel/user_book_matrix.npz')\n    emotion_matrix = load_npz('/kaggle/input/readyforrecommendationmodel/emotion_matrix.npz')\n    book_embeddings = load_npz('/kaggle/input/readyforrecommendationmodel/avg_embeddings_matrix.npz')\n    \n    return user_book_matrix, emotion_matrix, book_embeddings","metadata":{"execution":{"iopub.status.busy":"2024-07-16T22:41:08.902011Z","iopub.execute_input":"2024-07-16T22:41:08.902428Z","iopub.status.idle":"2024-07-16T22:41:09.006862Z","shell.execute_reply.started":"2024-07-16T22:41:08.902402Z","shell.execute_reply":"2024-07-16T22:41:09.005299Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class NCFDataset(Dataset):\n    def __init__(self, interactions, emotions, review_embeddings):\n        self.users, self.items = interactions.nonzero()\n        self.ratings = interactions[self.users, self.items].A1\n        self.emotions = emotions[self.items].toarray()  # Convert to dense array\n        self.review_embeddings = review_embeddings[self.items].toarray()  # Convert to dense array\n\n    def __len__(self):\n        return len(self.users)\n    \n    def __getitem__(self, idx):\n        return (self.users[idx], self.items[idx], self.emotions[idx], \n                self.review_embeddings[idx], self.ratings[idx])","metadata":{"execution":{"iopub.status.busy":"2024-07-16T22:41:09.008558Z","iopub.execute_input":"2024-07-16T22:41:09.009811Z","iopub.status.idle":"2024-07-16T22:41:09.022332Z","shell.execute_reply.started":"2024-07-16T22:41:09.009758Z","shell.execute_reply":"2024-07-16T22:41:09.020659Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class NCF(nn.Module):\n    def __init__(self, num_users, num_items, num_emotions, review_embedding_dim, \n                 embedding_dim=64, mlp_dims=[256, 128, 64], dropout=0.2):\n        super(NCF, self).__init__()\n        \n        # Embedding layers\n        self.user_embedding_mf = nn.Embedding(num_users, embedding_dim)\n        self.item_embedding_mf = nn.Embedding(num_items, embedding_dim)\n        self.user_embedding_mlp = nn.Embedding(num_users, embedding_dim)\n        self.item_embedding_mlp = nn.Embedding(num_items, embedding_dim)\n        self.emotion_embedding = nn.Embedding(num_emotions, embedding_dim)\n        \n        # MF layer\n        self.mf_output = embedding_dim\n        \n        # MLP layers\n        self.mlp = nn.ModuleList()\n        input_dim = embedding_dim * 3 + review_embedding_dim  # user + item + emotion + review\n        mlp_dims = [input_dim] + mlp_dims\n        for i in range(len(mlp_dims) - 1):\n            self.mlp.append(nn.Linear(mlp_dims[i], mlp_dims[i+1]))\n            self.mlp.append(nn.ReLU())\n            self.mlp.append(nn.BatchNorm1d(mlp_dims[i+1]))\n            self.mlp.append(nn.Dropout(dropout))\n        \n        # Final prediction layer\n        self.final = nn.Linear(self.mf_output + mlp_dims[-1], 1)\n        \n    def forward(self, user_indices, item_indices, emotion_indices, review_embeddings):\n        # MF component\n        user_embedding_mf = self.user_embedding_mf(user_indices)\n        item_embedding_mf = self.item_embedding_mf(item_indices)\n        mf_vector = torch.mul(user_embedding_mf, item_embedding_mf)\n        \n        # MLP component\n        user_embedding_mlp = self.user_embedding_mlp(user_indices)\n        item_embedding_mlp = self.item_embedding_mlp(item_indices)\n        emotion_embedding = self.emotion_embedding(emotion_indices)\n        emotion_embedding = self.emotion_embedding(emotion_indices).mean(dim=1)  # Adjusted for mean over emotions\n        mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp, emotion_embedding, review_embeddings], dim=-1)\n \n        for layer in self.mlp:\n            mlp_vector = layer(mlp_vector)\n        \n        # Combine MF and MLP\n        combined = torch.cat([mf_vector, mlp_vector], dim=-1)\n        \n        # Final prediction\n        prediction = self.final(combined)\n        \n        return prediction.squeeze()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T22:41:09.026719Z","iopub.execute_input":"2024-07-16T22:41:09.027579Z","iopub.status.idle":"2024-07-16T22:41:09.053309Z","shell.execute_reply.started":"2024-07-16T22:41:09.027522Z","shell.execute_reply":"2024-07-16T22:41:09.052093Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_data_info(user_item_interactions, emotion_labels):\n    print(user_item_interactions.shape)\n    # Get the number of unique users and items\n    num_users = user_item_interactions.shape[0]\n    num_items = user_item_interactions.shape[1]\n    \n    # Get the number of unique emotions\n    num_emotions = emotion_labels.shape[1]\n    \n    return num_users, num_items, num_emotions","metadata":{"execution":{"iopub.status.busy":"2024-07-16T22:41:09.055257Z","iopub.execute_input":"2024-07-16T22:41:09.056275Z","iopub.status.idle":"2024-07-16T22:41:09.068212Z","shell.execute_reply.started":"2024-07-16T22:41:09.056225Z","shell.execute_reply":"2024-07-16T22:41:09.066811Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def custom_collate_fn(batch):\n    users, items, emotions, review_embeddings, ratings = zip(*batch)\n    \n    users = torch.tensor(users, dtype=torch.long)\n    items = torch.tensor(items, dtype=torch.long)\n    emotions = torch.tensor(emotions, dtype=torch.long)\n    review_embeddings = torch.tensor(review_embeddings, dtype=torch.float)\n    ratings = torch.tensor(ratings, dtype=torch.float)\n    \n    # Move tensors to device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    users = users.to(device)\n    items = items.to(device)\n    emotions = emotions.to(device)\n    review_embeddings = review_embeddings.to(device)\n    ratings = ratings.to(device)\n    \n    return users, items, emotions, review_embeddings, ratings","metadata":{"execution":{"iopub.status.busy":"2024-07-16T22:41:09.070050Z","iopub.execute_input":"2024-07-16T22:41:09.070343Z","iopub.status.idle":"2024-07-16T22:41:09.078251Z","shell.execute_reply.started":"2024-07-16T22:41:09.070320Z","shell.execute_reply":"2024-07-16T22:41:09.077396Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"embedding_dim = 32\nreview_embedding_dim = 100\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 10\n\n# Assume we have these functions to load and preprocess data\nuser_item_interactions, emotion_labels, review_embeddings = load_data()\nnum_users, num_items, num_emotions = get_data_info(user_item_interactions, emotion_labels)\n\n# Create dataset and dataloader\ndataset = NCFDataset(user_item_interactions, emotion_labels, review_embeddings)\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=custom_collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T22:41:09.079510Z","iopub.execute_input":"2024-07-16T22:41:09.079875Z","iopub.status.idle":"2024-07-16T22:41:12.445938Z","shell.execute_reply.started":"2024-07-16T22:41:09.079842Z","shell.execute_reply":"2024-07-16T22:41:12.445045Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"(575887, 74298)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize model, loss, and optimizer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = NCF(num_users, num_items, num_emotions, \n            embedding_dim=32, \n            review_embedding_dim=100, \n            mlp_dims=[256, 128, 64], \n            dropout=0.2).to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T22:41:12.447077Z","iopub.execute_input":"2024-07-16T22:41:12.447413Z","iopub.status.idle":"2024-07-16T22:41:14.640496Z","shell.execute_reply.started":"2024-07-16T22:41:12.447382Z","shell.execute_reply":"2024-07-16T22:41:14.639405Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoint.pt'):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n\n    def __call__(self, val_loss, model):\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            if self.verbose:\n                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        \"\"\"Saves model when validation loss decreases.\"\"\"\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss","metadata":{"execution":{"iopub.status.busy":"2024-07-16T22:41:14.641753Z","iopub.execute_input":"2024-07-16T22:41:14.642181Z","iopub.status.idle":"2024-07-16T22:41:14.652298Z","shell.execute_reply.started":"2024-07-16T22:41:14.642154Z","shell.execute_reply":"2024-07-16T22:41:14.651308Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Initialize EarlyStopping object\nearly_stopping = EarlyStopping(patience=5, verbose=True, path='/kaggle/working/NCF_model.pth')\n\nfor epoch in range(num_epochs):\n    model.train()\n    for user, item, emotion, review_emb, rating in train_loader:\n        optimizer.zero_grad()\n        prediction = model(user, item, emotion, review_emb)\n        loss = criterion(prediction, rating)\n        loss.backward()\n        optimizer.step()\n\n    # Validation step\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for user, item, emotion, review_emb, rating in test_loader:\n            prediction = model(user, item, emotion, review_emb)\n            loss = criterion(prediction, rating)\n            val_loss += loss.item() * user.size(0)\n        val_loss /= len(test_loader.dataset)\n    \n    # Print validation loss\n    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}')\n\n    # Update early stopping object\n    early_stopping(val_loss, model)\n\n    # Check if early stopping criterion is met\n    if early_stopping.early_stop:\n        print(\"Early stopping\")\n        break\n\n# Load the best model checkpoint\nmodel.load_state_dict(torch.load('/kaggle/working/NCF_model.pth'))","metadata":{"execution":{"iopub.status.busy":"2024-07-16T22:41:14.653853Z","iopub.execute_input":"2024-07-16T22:41:14.654150Z","iopub.status.idle":"2024-07-16T22:41:51.442356Z","shell.execute_reply.started":"2024-07-16T22:41:14.654126Z","shell.execute_reply":"2024-07-16T22:41:51.440997Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1254461971.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n  emotions = torch.tensor(emotions, dtype=torch.long)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model(user, item, emotion, review_emb)\n\u001b[1;32m      9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(prediction, rating)\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Validation step\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}